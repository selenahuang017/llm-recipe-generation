{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c1ced21",
   "metadata": {},
   "source": [
    "# Imports/Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a429ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Settings:\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f03f99",
   "metadata": {},
   "source": [
    "## Usage:\n",
    "\n",
    "OpenAI Model Information:\n",
    "* https://huggingface.co/openai/gpt-oss-20b\n",
    "* https://huggingface.co/openai/gpt-oss-120b\n",
    "\n",
    "\n",
    "Here's an example of how to query in python:\n",
    "\n",
    "```python\n",
    "data = {'model': 'gpt-oss:120b', 'prompt': 'Give me a haiku about low effort memes'}\n",
    "url = 'https://ollama.loweffort.meme/api/generate'\n",
    "\n",
    "with requests.post(url, json=data, stream=True, verify=False) as r:\n",
    "    for line in r.iter_lines():\n",
    "        if line:\n",
    "            j = json.loads(line)\n",
    "            if 'response' in j:\n",
    "                print(j['response'], end='')\n",
    "```\n",
    "\n",
    "And here is an example on curling via terminal:\n",
    "\n",
    "```\n",
    "curl -k https://ollama.loweffort.meme/api/generate \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "        \"model\": \"gpt-oss:120b\",\n",
    "        \"prompt\": \"Give me a haiku about low effort memes\"\n",
    "      }'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cfc92f",
   "metadata": {},
   "source": [
    "# Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9132b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ollama_response(data):\n",
    "    \"\"\"\n",
    "    Sends a streaming generation request to your local Ollama API.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The JSON payload to send to Ollama, e.g.\n",
    "            {'model': 'gpt-oss:20b', 'prompt': 'Write a haiku about low effort memes'}\n",
    "\n",
    "    Returns:\n",
    "        str: The complete generated text response from the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    url = 'https://ollama.loweffort.meme/api/generate'\n",
    "    output = []\n",
    "\n",
    "    with requests.post(url, json=data, stream=True, verify=False) as r:\n",
    "        for line in r.iter_lines():\n",
    "            if line:\n",
    "                j = json.loads(line)\n",
    "                if 'response' in j:\n",
    "\n",
    "                    # Stream output live:\n",
    "                    print(j['response'], end='')  \n",
    "                    output.append(j['response'])\n",
    "                    \n",
    "    # Newline after streaming\n",
    "    print()  \n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e00d406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low effort memes roll wild  \n",
      "Meme hearts glitch, no spark, shrug all  \n",
      "Flicker of a tiny laugh\n"
     ]
    }
   ],
   "source": [
    "# Identify model and prompt:\n",
    "model = 'gpt-oss:20b'\n",
    "prompt = 'Give me a haiku about low effort memes'\n",
    "\n",
    "# Call function:\n",
    "data = {'model': model, 'prompt': prompt,}\n",
    "_ = generate_ollama_response(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "en605645",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
