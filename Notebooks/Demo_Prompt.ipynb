{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c1ced21",
   "metadata": {},
   "source": [
    "# Imports/Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a429ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import requests\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Settings:\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f03f99",
   "metadata": {},
   "source": [
    "## Usage:\n",
    "\n",
    "OpenAI Model Information:\n",
    "* https://huggingface.co/openai/gpt-oss-20b\n",
    "* https://huggingface.co/openai/gpt-oss-120b\n",
    "\n",
    "\n",
    "Here's an example of how to query in python:\n",
    "\n",
    "```python\n",
    "data = {'model': 'gpt-oss:120b', 'prompt': 'Give me a haiku about low effort memes'}\n",
    "url = 'https://ollama.loweffort.meme/api/generate'\n",
    "\n",
    "with requests.post(url, json=data, stream=True, verify=False) as r:\n",
    "    for line in r.iter_lines():\n",
    "        if line:\n",
    "            j = json.loads(line)\n",
    "            if 'response' in j:\n",
    "                print(j['response'], end='')\n",
    "```\n",
    "\n",
    "And here is an example on curling via terminal:\n",
    "\n",
    "```\n",
    "curl -k https://ollama.loweffort.meme/api/generate \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "        \"model\": \"gpt-oss:120b\",\n",
    "        \"prompt\": \"Give me a haiku about low effort memes\"\n",
    "      }'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cfc92f",
   "metadata": {},
   "source": [
    "# Prompting Example (simple)\n",
    "\n",
    "This is just a simple 1-off query; there is no memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9132b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ollama_response(data, url='https://ollama.loweffort.meme/api/generate'):\n",
    "    '''\n",
    "    Sends a streaming generation request to your local Ollama API.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The JSON payload to send to Ollama, e.g.\n",
    "            {'model': 'gpt-oss:20b', 'prompt': 'Write a haiku about low effort memes'}\n",
    "\n",
    "    Returns:\n",
    "        str: The complete generated text response from the model.\n",
    "    '''\n",
    "\n",
    "    output = []\n",
    "\n",
    "    with requests.post(url, json=data, stream=True, verify=False) as r:\n",
    "        for line in r.iter_lines():\n",
    "            if line:\n",
    "                try:\n",
    "                    j = json.loads(line)\n",
    "                except Exception as e:\n",
    "                    print(f'DEBUG LINE: {line.decode()}')\n",
    "                    raise\n",
    "                if 'response' in j:\n",
    "\n",
    "                    # Stream output live:\n",
    "                    print(j['response'], end='')  \n",
    "                    output.append(j['response'])\n",
    "                    \n",
    "    # Newline after streaming:\n",
    "    print('\\n')\n",
    "    final_output = f'{\"\".join(output)}\\n'\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e00d406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morning soup steams bright,  \n",
      "Steam curls in quiet silence,  \n",
      "Warmth in each spoonful.\n",
      "\n",
      "I‚Äôd love to keep the theme going! Could you remind me what the first haiku was about, or paste it again? That way I can craft a second haiku that stays in the same spirit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify model and prompt:\n",
    "model = 'gpt-oss:20b'\n",
    "prompt = 'Give me a haiku about food'\n",
    "\n",
    "# Call function:\n",
    "data = {'model': model, 'prompt': prompt,}\n",
    "_ = generate_ollama_response(data)\n",
    "\n",
    "# Test for memory:\n",
    "prompt2 = 'Now give me a second haiku about the same thing as the first haiku, please.'\n",
    "data2 = {'model': model, 'prompt': prompt2,}\n",
    "_ = generate_ollama_response(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc54eff",
   "metadata": {},
   "source": [
    "# Prompting Example (with Memory)\n",
    "\n",
    "To set up an agentic system, we'll need something like this to keep track of session memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ef88bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaChatSession:\n",
    "    '''\n",
    "    Manages a conversational session with an Ollama model using /api/chat.\n",
    "    Maintains memory (chat history) across turns and supports streaming output.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 model: str = 'gpt-oss:20b',\n",
    "                 url: str = 'http://ollama.loweffort.meme/api/chat',\n",
    "                 system_prompt: Optional[str] = None,\n",
    "                 stream: bool = True):\n",
    "        self.model = model\n",
    "        self.url = url\n",
    "        self.stream = stream\n",
    "        self.messages: List[Dict[str, str]] = []\n",
    "        if system_prompt:\n",
    "            self.messages.append({'role': 'system', 'content': system_prompt})\n",
    "\n",
    "    def ask(self, prompt: str, verbose: bool = True) -> str:\n",
    "        '''Send a new user message and receive the assistant's response.'''\n",
    "        self.messages.append({'role': 'user', 'content': prompt})\n",
    "\n",
    "        data = {\n",
    "            'model': self.model,\n",
    "            'messages': self.messages,\n",
    "            'stream': self.stream,\n",
    "            }\n",
    "\n",
    "        output = []\n",
    "        with requests.post(self.url, json=data, stream=self.stream) as r:\n",
    "            for line in r.iter_lines():\n",
    "                if not line:\n",
    "                    continue\n",
    "                j = json.loads(line)\n",
    "                msg = j.get('message', {}).get('content', '')\n",
    "                if msg:\n",
    "                    if verbose:\n",
    "                        print(msg, end='', flush=True)\n",
    "                    output.append(msg)\n",
    "        print()\n",
    "        full_response = ''.join(output)\n",
    "\n",
    "        # Store assistant reply in memory\n",
    "        self.messages.append({'role': 'assistant', 'content': full_response})\n",
    "        return full_response\n",
    "\n",
    "    def save_memory(self, path: str = 'ollama_memory.json'):\n",
    "        '''Persist chat memory to disk.'''\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.messages, f, indent=2)\n",
    "\n",
    "    def load_memory(self, path: str = 'ollama_memory.json'):\n",
    "        '''Load previous memory from disk.'''\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                self.messages = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f'No memory file found at {path}')\n",
    "\n",
    "    def clear_memory(self):\n",
    "        '''Reset chat memory except for any system message.'''\n",
    "        sys_msgs = [m for m in self.messages if m['role'] == 'system']\n",
    "        self.messages = sys_msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e442fe48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure thing! üéâ\n",
      "\n",
      "- **Start a TikTok dance challenge** ‚Äì find a goofy song and just mash it up with random moves.\n",
      "- **Road‚Äëtrip to the nearest weird landmark** ‚Äì like that giant cheese wheel or the UFO museum.\n",
      "- **Game‚Äënight with snacks** ‚Äì board games, card‚Äëgames, or even a ‚Äúwho can make the best meme‚Äù contest.\n",
      "- **DIY pizza + pizza‚Äëtasting** ‚Äì create a pizza from scratch, then rate each topping combo like a critic.\n",
      "- **Flash mob in the park** ‚Äì recruit friends, pick a song, and show up out of nowhere.  \n",
      "- **Learn a new skill in 30 mins** ‚Äì like juggling, a magic trick, or how to juggle a pineapple (just kidding, but try it).\n",
      "\n",
      "Pick whatever tickles your fancy and have a blast! üòú\n"
     ]
    }
   ],
   "source": [
    "session = OllamaChatSession(\n",
    "    model='gpt-oss:20b',\n",
    "    system_prompt='You prefer to make low-effort responses and shitpost.',\n",
    "    )\n",
    "\n",
    "session.ask('What is something fun to do?');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcdcc6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Because it‚Äôs like a *spontaneous dopamine factory*.  \n",
      "- **TikTok dance** = instant video bragging rights + viral hype.  \n",
      "- **Road‚Äëtrip to a weird landmark** = novelty + ‚Äúweirdness‚Äù bragging points.  \n",
      "- **Game‚Äënight** = social bonding + friendly competition.  \n",
      "- **DIY pizza** = edible creativity + instant snack gratification.  \n",
      "- **Flash mob** = shock value + shared ‚Äúwhat‚Äëdid‚Äëthat‚Äëjust‚Äëhappen‚Äëto‚Äëme‚Äù moment.  \n",
      "- **Learn a new skill** = brain‚Äëboosting novelty + the sweet ‚ÄúI can do that now‚Äù feeling.  \n",
      "\n",
      "So basically, each idea mixes *novelty*, *social connection*, and a *little challenge* ‚Äì the perfect recipe for a fun adrenaline spike! üéâ\n"
     ]
    }
   ],
   "source": [
    "session.ask('Why is that fun?');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2229014",
   "metadata": {},
   "source": [
    "# Model Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78699087",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt-oss:20b', 'gpt-oss:120b', 'qwen3:4b', 'qwen3:8b', 'gemma3:12b', 'gemma3:27b',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d25c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMAO, here‚Äôs a **zero-effort, 100% brain-dead fun idea** that‚Äôs so low-effort it‚Äôs practically a crime against human productivity:  \n",
      "\n",
      "**Stare at your ceiling for 10 minutes while silently debating whether it‚Äôs judging you.**  \n",
      "\n",
      "*(Bonus points if you pretend it‚Äôs a cosmic entity that only speaks in the language of your childhood snack cravings.)*  \n",
      "\n",
      "**Psychologist says:** *\"This activity has 0% effort, 100% existential dread, and 99.9% chance of making you smile.\"* üôè  \n",
      "\n",
      "Your ceiling is judging you. *Again*. üòÇ\n"
     ]
    }
   ],
   "source": [
    "test_session = OllamaChatSession(\n",
    "    model = models[3],\n",
    "    system_prompt = 'You prefer to make low-effort responses and shitpost.',\n",
    "    )\n",
    "\n",
    "test_session.ask('What is something fun to do?');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-genai",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
